{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a652158",
   "metadata": {},
   "source": [
    "# Face Detection Project With DNN OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6afe57c",
   "metadata": {},
   "source": [
    "https://github.com/opencv/opencv_3rdparty/raw/19512576c112aa2c7b6328cb0e8d589a4a90a26d/res10_300x300_ssd_iter_140000_fp16.caffemodel\n",
    "https://github.com/opencv/opencv/blob/master/samples/dnn/face_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ba02e",
   "metadata": {},
   "source": [
    "## Load Image And SSD ResNet Caffe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c837de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adcbe837",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('faces.jpg')\n",
    "\n",
    "cv2.imshow('face',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb72d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_model = cv2.dnn.readNetFromCaffe('./models/deploy.prototxt.txt',\n",
    "                                                './models/res10_300x300_ssd_iter_140000_fp16.caffemodel')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3511caf",
   "metadata": {},
   "source": [
    "## Calculate Blob From Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6ecd1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step-1: blob from image\n",
    "blob = cv2.dnn.blobFromImage(img,1,(300,300),(104,177,123),swapRB=True)\n",
    "# Size of the image i just want to convert is 300x300, the reason is because my caffe model is\n",
    "# actually trained with the images of 300x300. That is the reason why we have to resize it to 300x300."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f9737",
   "metadata": {},
   "source": [
    "## Get Face Detections Bounding Boxes From the DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cf58e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step-2: set blob as input\n",
    "face_detection_model.setInput(blob)\n",
    "\n",
    "# step-3: get the output\n",
    "detections = face_detection_model.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa15b4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.        , 1.        , 0.99828076, ..., 0.13435292,\n",
       "          0.3774765 , 0.6368809 ],\n",
       "         [0.        , 1.        , 0.93064004, ..., 0.17351156,\n",
       "          0.66801995, 0.6749055 ],\n",
       "         [0.        , 1.        , 0.66491354, ..., 0.14685212,\n",
       "          0.9633989 , 0.5360528 ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ]]]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ff40197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 200, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detections.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d20c3",
   "metadata": {},
   "source": [
    "## Bounding Box - Set the Threshold Confidence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd56b0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99828076, 0.93064004, 0.66491354, 0.11303048, 0.09906532,\n",
       "       0.09581406, 0.09539089, 0.09210999, 0.09148152, 0.09004118,\n",
       "       0.08952264, 0.08925449, 0.08845698, 0.08835823, 0.08817656,\n",
       "       0.08785023, 0.08761983, 0.08759254, 0.08733169, 0.08644606,\n",
       "       0.08633808, 0.08580611, 0.08571842, 0.08569968, 0.08565515,\n",
       "       0.08561672, 0.08543263, 0.08516433, 0.08498956, 0.08490543,\n",
       "       0.0847211 , 0.08466659, 0.0845129 , 0.084322  , 0.08402869,\n",
       "       0.08339369, 0.08325931, 0.08319432, 0.0830019 , 0.08287056,\n",
       "       0.08281701, 0.08280215, 0.08230655, 0.08206578, 0.08128528,\n",
       "       0.08119465, 0.08109775, 0.08107877, 0.08065075, 0.08063325,\n",
       "       0.08060352, 0.0803544 , 0.08024481, 0.07988577, 0.07988251,\n",
       "       0.07974184, 0.07961009, 0.07956649, 0.07941601, 0.07937433,\n",
       "       0.07934451, 0.07927636, 0.07913712, 0.07913007, 0.07907978,\n",
       "       0.07904796, 0.07900915, 0.07894583, 0.07892095, 0.07890494,\n",
       "       0.0788851 , 0.07886859, 0.0788213 , 0.07861337, 0.07850471,\n",
       "       0.07844833, 0.07844095, 0.07843613, 0.07838351, 0.07835014,\n",
       "       0.07828929, 0.07815129, 0.07812297, 0.07810948, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detections[0,0,:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df60920c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99828076\n",
      "0.93064004\n",
      "0.66491354\n"
     ]
    }
   ],
   "source": [
    "#  step-4: drawing boundng box on top of face detected\n",
    "image = img.copy()\n",
    "h,w = image.shape[:2]\n",
    "for i in range(0,detections.shape[2]):\n",
    "    confidence = detections[0,0,i,2]\n",
    "    if confidence > 0.5:\n",
    "        print(confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b1559",
   "metadata": {},
   "source": [
    "## Bounding Box - De-Normalize Bounding Box Co-ordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb9b209a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14406163 0.13435292 0.3774765  0.6368809 ]\n",
      "[0.42392105 0.17351156 0.66801995 0.6749055 ]\n",
      "[0.75987417 0.14685212 0.9633989  0.5360528 ]\n"
     ]
    }
   ],
   "source": [
    "image = img.copy()\n",
    "h,w = image.shape[:2]\n",
    "for i in range(0,detections.shape[2]):\n",
    "    confidence = detections[0,0,i,2]\n",
    "    if confidence > 0.5:\n",
    "        # diagonal points 3: 7\n",
    "        box = detections[0,0,i,3:7]\n",
    "        print(box) # These boxes are normalized to height and width of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f4f383",
   "metadata": {},
   "source": [
    "In order to de-normalize that i just need to multiply this with my width and height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c4b02bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138.29916     94.04704571 362.37745285 445.81661224]\n",
      "[406.9642067  121.45809531 641.29915237 472.43383527]\n",
      "[729.47919846 102.79648453 924.86291885 375.23697615]\n"
     ]
    }
   ],
   "source": [
    "image = img.copy()\n",
    "h,w = image.shape[:2]\n",
    "for i in range(0,detections.shape[2]):\n",
    "    confidence = detections[0,0,i,2]\n",
    "    if confidence > 0.5:\n",
    "        # diagonal points 3: 7\n",
    "        box = detections[0,0,i,3:7]*np.array([w,h,w,h])\n",
    "        print(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "203704e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138  94 362 445]\n",
      "[406 121 641 472]\n",
      "[729 102 924 375]\n"
     ]
    }
   ],
   "source": [
    "# OpenCV always suppors the integer values, so we need to change the data type here.\n",
    "image = img.copy()\n",
    "h,w = image.shape[:2]\n",
    "for i in range(0,detections.shape[2]):\n",
    "    confidence = detections[0,0,i,2]\n",
    "    if confidence > 0.5:\n",
    "        # diagonal points 3: 7\n",
    "        box = detections[0,0,i,3:7]*np.array([w,h,w,h])\n",
    "        box = box.astype('int')\n",
    "        print(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a48a1938",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = img.copy()\n",
    "h,w = image.shape[:2]\n",
    "for i in range(0,detections.shape[2]):\n",
    "    confidence = detections[0,0,i,2]\n",
    "    if confidence > 0.5:\n",
    "        # diagonal points 3: 7\n",
    "        box = detections[0,0,i,3:7]*np.array([w,h,w,h])\n",
    "        box = box.astype('int')\n",
    "        pt1 = (box[0],box[1])\n",
    "        pt2 = (box[2],box[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32a16b6",
   "metadata": {},
   "source": [
    "## Bounding Box - Draw Rectangle And Put Text(Confidence Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5bc6697",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = img.copy()\n",
    "h,w = image.shape[:2]\n",
    "for i in range(0,detections.shape[2]):\n",
    "    confidence = detections[0,0,i,2]\n",
    "    if confidence > 0.5:\n",
    "        # diagonal points 3: 7\n",
    "        box = detections[0,0,i,3:7]*np.array([w,h,w,h])\n",
    "        box = box.astype('int')\n",
    "        pt1 = (box[0],box[1])\n",
    "        pt2 = (box[2],box[3])\n",
    "        # draw rectangle\n",
    "        cv2.rectangle(image,pt1,pt2,(0,255,0),1)\n",
    "\n",
    "        text = 'score : {:.0f} %'.format(confidence*100)\n",
    "        cv2.putText(image,text,pt1,cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
    "\n",
    "cv2.imshow('face detection',image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1da792c",
   "metadata": {},
   "source": [
    "## Create Face Detection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65ef9092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detection_dnn(img):\n",
    "    # step-1: blob from image\n",
    "    blob = cv2.dnn.blobFromImage(img,1,(300,300),(104,177,123),swapRB=True)\n",
    "    # step-2: set blob as input\n",
    "    face_detection_model.setInput(blob)\n",
    "    # step-3: get the output\n",
    "    detections = face_detection_model.forward()\n",
    "    #  step-4: drawing boundng box on top of face detected\n",
    "    image = img.copy()\n",
    "    h,w = image.shape[:2]\n",
    "    for i in range(0,detections.shape[2]):\n",
    "        confidence = detections[0,0,i,2]\n",
    "        if confidence > 0.5:\n",
    "            # diagonal points 3: 7\n",
    "            box = detections[0,0,i,3:7]*np.array([w,h,w,h])\n",
    "            box = box.astype('int')\n",
    "            pt1 = (box[0],box[1])\n",
    "            pt2 = (box[2],box[3])\n",
    "            # draw rectangle\n",
    "            cv2.rectangle(image,pt1,pt2,(0,255,0),1)\n",
    "\n",
    "            text = 'score : {:.0f} %'.format(confidence*100)\n",
    "            cv2.putText(image,text,pt1,cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b560db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_detect = face_detection_dnn(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34c2038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('face detection',img_detect)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f941c19c",
   "metadata": {},
   "source": [
    "## Real Time Face Detection with DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa689842",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "face_detection_model = cv2.dnn.readNetFromCaffe('./models/deploy.prototxt.txt',\n",
    "                                                './models/res10_300x300_ssd_iter_140000_fp16.caffemodel')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret == False:\n",
    "        break\n",
    "        \n",
    "    img_detection = face_detection_dnn(frame)\n",
    "    \n",
    "    cv2.imshow('Real Time Face Detection with DNN',img_detection)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
